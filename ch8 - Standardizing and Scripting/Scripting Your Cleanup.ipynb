{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e664c773",
   "metadata": {},
   "source": [
    "## Our Script so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7025e0aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mwelevel\n",
      "mnweight\n",
      "wscoreu\n",
      "windex5u\n",
      "wscorer\n",
      "windex5r\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "import dataset\n",
    "import pickle\n",
    "\n",
    "data_rdr = reader(open('mn.csv', 'r', encoding='utf-8'))\n",
    "header_rdr = reader(open('mn_headers_updated.csv', 'r', encoding='utf-8'))\n",
    "\n",
    "data_rows = [d for d in data_rdr]\n",
    "header_rows = [h for h in header_rdr if h[0] in data_rows[0]]\n",
    "\n",
    "all_short_headers = [h[0] for h in header_rows]\n",
    "skip_index = []\n",
    "final_header_rows = []\n",
    "\n",
    "for header in data_rows[0]:\n",
    "    if header not in all_short_headers:\n",
    "        print(header)\n",
    "        index = data_rows[0].index(header)\n",
    "        if index not in skip_index:\n",
    "            skip_index.append(index)\n",
    "    else:\n",
    "        for head in header_rows:\n",
    "            if head[0] == header:\n",
    "                final_header_rows.append(head)\n",
    "                break\n",
    "new_data = []\n",
    "\n",
    "for row in data_rows[1:]:\n",
    "    new_row = []\n",
    "    \n",
    "    for i, d in enumerate(row):\n",
    "        if i not in skip_index:\n",
    "            new_row.append(d)\n",
    "    new_data.append(new_row)\n",
    "\n",
    "zipped_data = []\n",
    "for drow in new_data:\n",
    "    zipped_data.append(zip(final_header_rows, drow))\n",
    "    \n",
    "# Save the list to a file\n",
    "with open('zipped_data.pkl', 'wb') as file:\n",
    "    pickle.dump(zipped_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9717a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list from the file\n",
    "def load_zipped_data():\n",
    "    with open('zipped_data.pkl', 'rb') as file:\n",
    "        zipped_data = pickle.load(file)\n",
    "        return zipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4f14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for missing\n",
    "zipped_data = load_zipped_data()\n",
    "for x in zipped_data[0]:\n",
    "    if not x[1]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4e8b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "#look for duplicates\n",
    "\n",
    "set_of_keys = set()\n",
    "zipped_data = load_zipped_data()\n",
    "for x in zipped_data:\n",
    "    unique_key = ''\n",
    "    for i, row in enumerate(x):\n",
    "        if i in [0, 1, 2]:\n",
    "            unique_key += str(row[1])+'-'\n",
    "    set_of_keys.add(unique_key[:-1])\n",
    "\n",
    "uniques = []\n",
    "zipped_data = load_zipped_data()\n",
    "for x in zipped_data:\n",
    "    unique_key = ''\n",
    "    for i, row in enumerate(x):\n",
    "        if i in [0, 1, 2]:\n",
    "            unique_key += str(row[1])+'-'\n",
    "    if not set_of_keys.remove(unique_key[:-1]):\n",
    "        uniques.append(x)\n",
    "        \n",
    "print(set_of_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f22735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to db\n",
    "zipped_data = load_zipped_data()\n",
    "db = dataset.connect('sqlite:///data_wrangling.db')\n",
    "table = db['unicef_survey']\n",
    "\n",
    "for row_num, data in enumerate(zipped_data):\n",
    "    for question, answer in data:\n",
    "        data_dict = {\n",
    "            'question': question[1],\n",
    "            'question_code': question[0],\n",
    "            'answer': answer,\n",
    "            'response_number': row_num,\n",
    "            'survery': 'mn',\n",
    "        }\n",
    "    table.insert(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7b041",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b7a57",
   "metadata": {},
   "source": [
    "## Refactoring the previous Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08bb85",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240741cf",
   "metadata": {},
   "source": [
    "### We can see most of our code is flat, meaning we don't have nested levels of importance. Much of the code and functions sit without identation or documentation in the file. It's not well abstracted, and the variable names are unclear. let's start working on parts of that, beginning at the top. The first two sets of lines repeat each other. Let's write a function to do that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a250818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import pickle\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06b6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(file_name):\n",
    "    rdr = reader(open(file_name, 'r', encoding='utf-8'))\n",
    "    return [row for row in rdr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb6f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_mismatches(header_rows, data_rows):\n",
    "    all_short_headers = [h[0] for h in header_rows]\n",
    "    skip_index = []\n",
    "    final_header_rows = []\n",
    "\n",
    "    for header in data_rows[0]:\n",
    "        if header not in all_short_headers:\n",
    "            print(header)\n",
    "            index = data_rows[0].index(header)\n",
    "            if index not in skip_index:\n",
    "                skip_index.append(index)\n",
    "        else:\n",
    "            for head in header_rows:\n",
    "                if head[0] == header:\n",
    "                    final_header_rows.append(head)\n",
    "                    break\n",
    "    return skip_index, final_header_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec66f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_data(headers, data):\n",
    "    zipped_data = []\n",
    "    for drow in data:\n",
    "        zipped_data.append(zip(headers, drow))\n",
    "    return zipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92e8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zipped_data(final_header_rows, data_rows, skip_index):\n",
    "    new_data = []\n",
    "\n",
    "    for row in data_rows[1:]:\n",
    "        new_row = []\n",
    "\n",
    "        for index, data in enumerate(row):\n",
    "            if index not in skip_index:\n",
    "                new_row.append(data)\n",
    "        new_data.append(new_row)\n",
    "    zipped_data = zip_data(final_header_rows, new_data)\n",
    "    # Save the list to a file\n",
    "    with open('zipped_data.pkl', 'wb') as file:\n",
    "        pickle.dump(zipped_data, file)\n",
    "        \n",
    "    return zipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f4ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list from the file\n",
    "def load_zipped_data():\n",
    "    with open('zipped_data.pkl', 'rb') as file:\n",
    "        zipped_data = pickle.load(file)\n",
    "        return zipped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96a3dc",
   "metadata": {},
   "source": [
    "### with our new functions, we were able to preserve our code, clear up some variable names, and add a helper function to zip headers with rows of data and return the list of zipped data. The code is clearer and broken up more appropriately. We're going to continue to apply the same logic to the rest of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7ad135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_data(zipped_data):\n",
    "    missing_count = 0\n",
    "    for row in zipped_data:\n",
    "        for question, answer in row:\n",
    "            if not answer:\n",
    "                missing_count += 1\n",
    "    return missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96894d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_data(zipped_data):\n",
    "    set_of_keys = set()\n",
    "    for x in zipped_data:\n",
    "        unique_key = ''\n",
    "        for i, row in enumerate(x):\n",
    "            if i in [0, 1, 2]:\n",
    "                unique_key += str(row[1])+'-'\n",
    "        set_of_keys.add(unique_key[:-1])\n",
    "\n",
    "    uniques = []\n",
    "    zipped_data = load_zipped_data()\n",
    "    for x in zipped_data:\n",
    "        unique_key = ''\n",
    "        for i, row in enumerate(x):\n",
    "            if i in [0, 1, 2]:\n",
    "                unique_key += str(row[1])+'-'\n",
    "        if not set_of_keys.remove(unique_key[:-1]):\n",
    "            uniques.append(x)\n",
    "\n",
    "    return uniques, len(set_of_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c53a7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_sqlitedb(db_file, zipped_data, survey_type):\n",
    "    db = dataset.connect(db_file)\n",
    "    table = db['unicef_survey']\n",
    "    all_rows = []\n",
    "\n",
    "    for row_num, data in enumerate(zipped_data):\n",
    "        for question, answer in data:\n",
    "            data_dict = {\n",
    "                'question': question[1],\n",
    "                'question_code': question[0],\n",
    "                'answer': answer,\n",
    "                'response_number': row_num,\n",
    "                'survery': 'mn',\n",
    "            }\n",
    "            all_rows.append(data_dict)\n",
    "            \n",
    "    table.insert_many(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff65fc",
   "metadata": {},
   "source": [
    "### We need to now work on re-creating how to use all these setps in a main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898d15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_rows = get_rows('mn.csv')\n",
    "    header_rows = get_rows('mn_headers_updated.csv')\n",
    "    skip_index, final_header_rows = eliminate_mismatches(header_rows, data_rows)\n",
    "    zipped_data = create_zipped_data(final_header_rows, data_rows, skip_index)\n",
    "    num_missing = find_missing_data(zipped_data)\n",
    "    uniques, num_dupes = find_duplicate_data(load_zipped_data())\n",
    "    if num_missing == 0 and num_dupes == 0:\n",
    "        save_to_sqlitedb('sqlite:///database/data_wrangling.db', load_zipped_data(), 'mn')\n",
    "    else:\n",
    "        error_msg = ''\n",
    "        if num_missing:\n",
    "            error_msg += 'We are missing {} values. '.format(num_missing)\n",
    "        if num_dupes:\n",
    "            error_msg += 'We have {} duplicates. '.format(num_dupes)\n",
    "        error_msg += 'Please have a look and fix!'\n",
    "        print(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301c3466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mwelevel\n",
      "mnweight\n",
      "wscoreu\n",
      "windex5u\n",
      "wscorer\n",
      "windex5r\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-wrangling",
   "language": "python",
   "name": "data-wrangling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
